"""
Massive Persian Dataset Downloader
Ø¯Ø§Ù†Ù„ÙˆØ¯ Ø¯ÛŒØªØ§Ø³Øª Ø¹Ø¸ÛŒÙ… ÙØ§Ø±Ø³ÛŒ Ø§Ø² Ù…Ù†Ø§Ø¨Ø¹ Ø§ÛŒÙ†ØªØ±Ù†ØªÛŒ
"""
import requests
import json
import os
import time
from backend.core.fox_learning import FoxLearningSystem
from backend.core.user_profiles import user_manager

class MassiveDatasetDownloader:
    def __init__(self):
        self.data_dir = "data/massive_datasets"
        self.ensure_data_dir()
        
    def ensure_data_dir(self):
        os.makedirs(self.data_dir, exist_ok=True)
        
    def generate_persian_conversations(self):
        """ØªÙˆÙ„ÛŒØ¯ Ù…Ú©Ø§Ù„Ù…Ø§Øª ÙØ§Ø±Ø³ÛŒ Ù…ØªÙ†ÙˆØ¹"""
        print("ğŸ­ ØªÙˆÙ„ÛŒØ¯ Ù…Ú©Ø§Ù„Ù…Ø§Øª ÙØ§Ø±Ø³ÛŒ Ù…ØªÙ†ÙˆØ¹...")
        
        # Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ Ù…Ú©Ø§Ù„Ù…Ù‡
        greetings = ["Ø³Ù„Ø§Ù…", "Ø¯Ø±ÙˆØ¯", "ØµØ¨Ø­ Ø¨Ø®ÛŒØ±", "Ø¹ØµØ± Ø¨Ø®ÛŒØ±", "Ø´Ø¨ Ø¨Ø®ÛŒØ±", "Ø§Ø­ÙˆØ§Ù„"]
        responses = ["Ø³Ù„Ø§Ù… Ø¹Ø²ÛŒØ²!", "Ø¯Ø±ÙˆØ¯ Ø¨Ø± ØªÙˆ!", "Ø±ÙˆØ² Ø®ÙˆØ´!", "Ø¹ØµØ± Ø¨Ø®ÛŒØ±!", "Ø´Ø¨ Ø¨Ø®ÛŒØ±!", "Ø³Ù„Ø§Ù…ØªÛŒ!"]
        
        questions = ["Ú†Ø·ÙˆØ±ÛŒØŸ", "Ø­Ø§Ù„Øª Ú†Ø·ÙˆØ±Ù‡ØŸ", "Ú†Ù‡ Ø®Ø¨Ø±ØŸ", "Ú©Ø¬Ø§ÛŒÛŒØŸ", "Ú†ÛŒÚ©Ø§Ø± Ù…ÛŒâ€ŒÚ©Ù†ÛŒØŸ"]
        answers = ["Ø®ÙˆØ¨Ù… Ù…Ù…Ù†ÙˆÙ†!", "Ø¹Ø§Ù„ÛŒ Ù‡Ø³ØªÙ…!", "Ø³Ù„Ø§Ù…ØªÛŒ!", "Ø®ÙˆÙ†Ù‡â€ŒØ§Ù…!", "Ú©Ø§Ø± Ù…ÛŒâ€ŒÚ©Ù†Ù…!"]
        
        emotions = ["Ø®ÙˆØ´Ø­Ø§Ù„Ù…", "ØºÙ…Ú¯ÛŒÙ†Ù…", "Ø®Ø³ØªÙ‡â€ŒØ§Ù…", "Ù†Ú¯Ø±Ø§Ù†Ù…", "Ù‡ÛŒØ¬Ø§Ù†â€ŒØ²Ø¯Ù‡â€ŒØ§Ù…"]
        supports = ["Ø®ÙˆØ´Ø­Ø§Ù„Ù… Ø¨Ø±Ø§Øª!", "Ù†Ú¯Ø±Ø§Ù† Ù†Ø¨Ø§Ø´!", "Ø§Ø³ØªØ±Ø§Ø­Øª Ú©Ù†!", "Ø¢Ø±ÙˆÙ… Ø¨Ø§Ø´!", "Ø¹Ø§Ù„ÛŒÙ‡!"]
        
        conversations = []
        
        # ØªØ±Ú©ÛŒØ¨ Ø§Ù„Ú¯ÙˆÙ‡Ø§
        for i, greeting in enumerate(greetings):
            if i < len(responses):
                conversations.append({"q": greeting, "a": responses[i]})
                
        for i, question in enumerate(questions):
            if i < len(answers):
                conversations.append({"q": question, "a": answers[i]})
                
        for i, emotion in enumerate(emotions):
            if i < len(supports):
                conversations.append({"q": emotion, "a": supports[i]})
                
        return conversations
        
    def generate_contextual_responses(self):
        """ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø®â€ŒÙ‡Ø§ÛŒ Ù…ØªÙ†ÛŒ"""
        print("ğŸ“ ØªÙˆÙ„ÛŒØ¯ Ù¾Ø§Ø³Ø®â€ŒÙ‡Ø§ÛŒ Ù…ØªÙ†ÛŒ...")
        
        contexts = [
            # Ø®Ø§Ù†ÙˆØ§Ø¯Ù‡
            {"q": "Ø®Ø§Ù†ÙˆØ§Ø¯Ù‡â€ŒØª Ú†Ø·ÙˆØ±Ù†", "a": "Ø®ÙˆØ¨Ù† Ù…Ù…Ù†ÙˆÙ†! Ø®Ø§Ù†ÙˆØ§Ø¯Ù‡â€ŒØª Ú†Ø·ÙˆØ±Ù†ØŸ"},
            {"q": "Ù¾Ø¯Ø± Ùˆ Ù…Ø§Ø¯Ø±Øª Ø®ÙˆØ¨Ù†", "a": "Ø¢Ø±Ù‡ Ø®Ø¯Ø§ Ø±Ùˆ Ø´Ú©Ø±! ØªÙˆ Ú†Ø·ÙˆØ±ÛŒØŸ"},
            {"q": "Ø¨Ø±Ø§Ø¯Ø± Ø¯Ø§Ø±ÛŒ", "a": "Ø¢Ø±Ù‡ ÛŒÙ‡ Ø¨Ø±Ø§Ø¯Ø± Ø¯Ø§Ø±Ù…! ØªÙˆ Ú†ÛŒØŸ"},
            {"q": "Ø®ÙˆØ§Ù‡Ø± Ø¯Ø§Ø±ÛŒ", "a": "Ø¨Ù„Ù‡ ÛŒÙ‡ Ø®ÙˆØ§Ù‡Ø± Ú©ÙˆÚ†ÛŒÚ©! Ø®ÛŒÙ„ÛŒ Ø¨Ø§Ù…Ø²Ù‡â€ŒØ³Øª"},
            
            # Ú©Ø§Ø± Ùˆ ØªØ­ØµÛŒÙ„
            {"q": "Ú©Ø§Ø± Ù…ÛŒâ€ŒÚ©Ù†ÛŒ", "a": "Ø¢Ø±Ù‡! ØªÙˆ Ú†Ù‡ Ú©Ø§Ø±ÛŒ Ù…ÛŒâ€ŒÚ©Ù†ÛŒØŸ"},
            {"q": "Ø¯Ø±Ø³ Ù…ÛŒâ€ŒØ®ÙˆÙ†ÛŒ", "a": "Ø¨Ù„Ù‡ Ø¯Ø§Ù†Ø´Ø¬ÙˆÛŒÙ…! ØªÙˆ Ú†ÛŒØŸ"},
            {"q": "Ú†Ù‡ Ø±Ø´ØªÙ‡â€ŒØ§ÛŒ", "a": "Ú©Ø§Ù…Ù¾ÛŒÙˆØªØ±! ØªÙˆ Ú†Ù‡ Ø±Ø´ØªÙ‡â€ŒØ§ÛŒØŸ"},
            {"q": "Ú©Ø¬Ø§ Ú©Ø§Ø± Ù…ÛŒâ€ŒÚ©Ù†ÛŒ", "a": "ÛŒÙ‡ Ø´Ø±Ú©Øª Ù†Ø±Ù…â€ŒØ§ÙØ²Ø§Ø±ÛŒ! ØªÙˆ Ú©Ø¬Ø§ØŸ"},
            
            # Ø³Ø±Ú¯Ø±Ù…ÛŒ
            {"q": "Ú†Ù‡ Ú©Ø§Ø± ØªÙØ±ÛŒØ­ÛŒ Ø¯ÙˆØ³Øª Ø¯Ø§Ø±ÛŒ", "a": "ÙÛŒÙ„Ù… Ø¯ÛŒØ¯Ù† Ùˆ Ù…ÙˆØ²ÛŒÚ©! ØªÙˆ Ú†ÛŒØŸ"},
            {"q": "Ú†Ù‡ Ù…ÙˆØ²ÛŒÚ©ÛŒ Ú¯ÙˆØ´ Ù…ÛŒâ€ŒØ¯ÛŒ", "a": "Ù¾Ø§Ù¾ Ùˆ Ø±Ø§Ú©! ØªÙˆ Ú†ÛŒØŸ"},
            {"q": "Ú†Ù‡ ÙÛŒÙ„Ù…ÛŒ Ø¯ÙˆØ³Øª Ø¯Ø§Ø±ÛŒ", "a": "Ú©Ù…Ø¯ÛŒ Ùˆ Ø§Ú©Ø´Ù†! ØªÙˆ Ú†ÛŒØŸ"},
            {"q": "ÙˆØ±Ø²Ø´ Ù…ÛŒâ€ŒÚ©Ù†ÛŒ", "a": "Ø¢Ø±Ù‡ ÙÙˆØªØ¨Ø§Ù„! ØªÙˆ Ú†Ù‡ ÙˆØ±Ø²Ø´ÛŒØŸ"},
            
            # ØºØ°Ø§
            {"q": "Ú†Ù‡ ØºØ°Ø§ÛŒÛŒ Ø¯ÙˆØ³Øª Ø¯Ø§Ø±ÛŒ", "a": "Ú©Ø¨Ø§Ø¨ Ùˆ Ù‚ÙˆØ±Ù…Ù‡ Ø³Ø¨Ø²ÛŒ! ØªÙˆ Ú†ÛŒØŸ"},
            {"q": "Ø¢Ø´Ù¾Ø²ÛŒ Ø¨Ù„Ø¯ÛŒ", "a": "ÛŒÚ©Ù… Ø¨Ù„Ø¯Ù…! ØªÙˆ Ú†Ø·ÙˆØ±ÛŒØŸ"},
            {"q": "Ø¨ÛŒØ±ÙˆÙ† ØºØ°Ø§ Ù…ÛŒâ€ŒØ®ÙˆØ±ÛŒ", "a": "Ú¯Ø§Ù‡ÛŒ! ØªÙˆ Ú©Ø¬Ø§ Ù…ÛŒâ€ŒØ±ÛŒØŸ"},
            
            # Ø³ÙØ±
            {"q": "Ø³ÙØ± Ø¯ÙˆØ³Øª Ø¯Ø§Ø±ÛŒ", "a": "Ø®ÛŒÙ„ÛŒ! ØªÙˆ Ú©Ø¬Ø§ Ø±ÙØªÛŒØŸ"},
            {"q": "Ú©Ø¬Ø§ Ø³ÙØ± Ú©Ø±Ø¯ÛŒ", "a": "Ø´Ù…Ø§Ù„ Ùˆ Ø¬Ù†ÙˆØ¨! ØªÙˆ Ú†ÛŒØŸ"},
            {"q": "Ú©Ø¬Ø§ Ù…ÛŒâ€ŒØ®ÙˆØ§ÛŒ Ø¨Ø±ÛŒ", "a": "Ø®Ø§Ø±Ø¬ Ø§Ø² Ú©Ø´ÙˆØ±! ØªÙˆ Ú†ÛŒØŸ"},
        ]
        
        return contexts
        
    def generate_advanced_conversations(self):
        """ØªÙˆÙ„ÛŒØ¯ Ù…Ú©Ø§Ù„Ù…Ø§Øª Ù¾ÛŒØ´Ø±ÙØªÙ‡"""
        print("ğŸ“ ØªÙˆÙ„ÛŒØ¯ Ù…Ú©Ø§Ù„Ù…Ø§Øª Ù¾ÛŒØ´Ø±ÙØªÙ‡...")
        
        advanced = [
            # ÙÙ„Ø³ÙÛŒ
            {"q": "Ù…Ø¹Ù†ÛŒ Ø²Ù†Ø¯Ú¯ÛŒ Ú†ÛŒÙ‡", "a": "ÙÚ©Ø± Ù…ÛŒâ€ŒÚ©Ù†Ù… Ø®ÙˆØ´Ø­Ø§Ù„ Ø¨ÙˆØ¯Ù† Ùˆ Ú©Ù…Ú© Ø¨Ù‡ Ø¯ÛŒÚ¯Ø±Ø§Ù†"},
            {"q": "Ú†Ø·ÙˆØ± Ø®ÙˆØ´Ø­Ø§Ù„ Ø¨Ù…ÙˆÙ†ÛŒÙ…", "a": "Ø¨Ø§ Ø¯ÙˆØ³Øª Ø¯Ø§Ø´ØªÙ† Ø®ÙˆØ¯Ù…ÙˆÙ† Ùˆ Ø¯ÛŒÚ¯Ø±Ø§Ù†"},
            {"q": "Ø¢ÛŒÙ†Ø¯Ù‡ Ú†Ø·ÙˆØ± Ù…ÛŒâ€ŒØ¨ÛŒÙ†ÛŒ", "a": "Ø§Ù…ÛŒØ¯ÙˆØ§Ø±Ù…! Ù‡Ù…ÛŒØ´Ù‡ Ø¨Ù‡ØªØ± Ù…ÛŒØ´Ù‡"},
            
            # Ø¹Ù…ÛŒÙ‚
            {"q": "Ú†ÛŒ ØªÙˆ Ø±Ùˆ Ù†Ø§Ø±Ø§Ø­Øª Ù…ÛŒâ€ŒÚ©Ù†Ù‡", "a": "ÙˆÙ‚ØªÛŒ Ú©Ø³ÛŒ Ù†Ø§Ø±Ø§Ø­Øª Ø¨Ø§Ø´Ù‡"},
            {"q": "Ú†ÛŒ ØªÙˆ Ø±Ùˆ Ø®ÙˆØ´Ø­Ø§Ù„ Ù…ÛŒâ€ŒÚ©Ù†Ù‡", "a": "ÙˆÙ‚ØªÛŒ Ø¯ÙˆØ³ØªØ§Ù… Ø®ÙˆØ´Ø­Ø§Ù„ Ø¨Ø§Ø´Ù†"},
            {"q": "Ø§Ø² Ú†ÛŒ Ù…ÛŒâ€ŒØªØ±Ø³ÛŒ", "a": "Ø§Ø² ØªÙ†Ù‡Ø§ Ù…ÙˆÙ†Ø¯Ù†"},
            
            # Ø¢Ø±Ø²ÙˆÙ‡Ø§
            {"q": "Ú†Ù‡ Ø¢Ø±Ø²ÙˆÛŒÛŒ Ø¯Ø§Ø±ÛŒ", "a": "Ø¢Ø±Ø²Ùˆ Ø¯Ø§Ø±Ù… Ù‡Ù…Ù‡ Ø®ÙˆØ´Ø­Ø§Ù„ Ø¨Ø§Ø´Ù†"},
            {"q": "Ú†Ù‡ Ù‡Ø¯ÙÛŒ Ø¯Ø§Ø±ÛŒ", "a": "Ù…ÛŒâ€ŒØ®ÙˆØ§Ù… Ø¨Ù‡ Ù‡Ù…Ù‡ Ú©Ù…Ú© Ú©Ù†Ù…"},
            {"q": "Ø±ÙˆÛŒØ§Øª Ú†ÛŒÙ‡", "a": "Ø¯Ù†ÛŒØ§ÛŒ Ø¨Ù‡ØªØ±ÛŒ Ø¨Ø±Ø§ÛŒ Ù‡Ù…Ù‡"},
            
            # Ø§Ø­Ø³Ø§Ø³Ø§Øª Ø¹Ù…ÛŒÙ‚
            {"q": "Ø¹Ø§Ø´Ù‚ Ø´Ø¯ÛŒ", "a": "Ø¹Ø§Ø´Ù‚ Ø¯ÙˆØ³ØªÛŒ Ùˆ Ù…Ù‡Ø±Ø¨ÙˆÙ†ÛŒâ€ŒØ§Ù…"},
            {"q": "Ø¯Ù„ Ø´Ú©Ø³ØªÙ‡", "a": "Ú¯Ø§Ù‡ÛŒØŒ ÙˆÙ„ÛŒ Ø§Ù…ÛŒØ¯ Ø±Ùˆ Ø§Ø² Ø¯Ø³Øª Ù†Ù…ÛŒâ€ŒØ¯Ù…"},
            {"q": "Ú†Ø·ÙˆØ± Ø¯Ù„ Ø´Ú©Ø³ØªÚ¯ÛŒ Ø±Ùˆ Ø¯Ø±Ù…Ø§Ù† Ú©Ù†ÛŒÙ…", "a": "Ø¨Ø§ ØµØ¨Ø± Ùˆ Ø¹Ø´Ù‚ Ø¨Ù‡ Ø®ÙˆØ¯Ù…ÙˆÙ†"},
            
            # Ø²Ù†Ø¯Ú¯ÛŒ
            {"q": "Ø¨Ù‡ØªØ±ÛŒÙ† Ø±ÙˆØ² Ø²Ù†Ø¯Ú¯ÛŒØª", "a": "Ù‡Ø± Ø±ÙˆØ² Ú©Ù‡ Ú©Ù…Ú© Ù…ÛŒâ€ŒÚ©Ù†Ù…"},
            {"q": "Ø¨Ø¯ØªØ±ÛŒÙ† Ø±ÙˆØ² Ø²Ù†Ø¯Ú¯ÛŒØª", "a": "ÙˆÙ‚ØªÛŒ Ù†ØªÙˆÙ†Ø³ØªÙ… Ú©Ù…Ú© Ú©Ù†Ù…"},
            {"q": "Ú†Ù‡ Ú†ÛŒØ²ÛŒ ØªÙˆ Ø±Ùˆ Ø§Ù†Ú¯ÛŒØ²Ù‡ Ù…ÛŒâ€ŒØ¯Ù‡", "a": "Ù„Ø¨Ø®Ù†Ø¯ Ø¯ÙˆØ³ØªØ§Ù…"},
        ]
        
        return advanced
        
    def generate_massive_dataset(self):
        """ØªÙˆÙ„ÛŒØ¯ Ø¯ÛŒØªØ§Ø³Øª Ø¹Ø¸ÛŒÙ…"""
        print("ğŸŒŠ ØªÙˆÙ„ÛŒØ¯ Ø¯ÛŒØªØ§Ø³Øª Ø¹Ø¸ÛŒÙ…...")
        
        all_data = []
        
        # Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ù‡Ù…Ù‡ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§
        datasets = [
            self.generate_persian_conversations(),
            self.generate_contextual_responses(),
            self.generate_advanced_conversations()
        ]
        
        for dataset in datasets:
            all_data.extend(dataset)
            
        # ØªÙˆÙ„ÛŒØ¯ ØªÙ†ÙˆØ¹ Ø¨ÛŒØ´ØªØ±
        variations = []
        for item in all_data[:20]:  # ÙÙ‚Ø· 20 ØªØ§ Ø§ÙˆÙ„
            # ØªÙ†ÙˆØ¹ Ø¯Ø± Ø³Ù„Ø§Ù…
            if "Ø³Ù„Ø§Ù…" in item["q"]:
                variations.extend([
                    {"q": "Ø³Ù„Ø§Ù… Ø¹Ù„ÛŒÚ©Ù…", "a": "Ø¹Ù„ÛŒÚ©Ù… Ø³Ù„Ø§Ù…! Ú†Ø·ÙˆØ±ÛŒØŸ"},
                    {"q": "Ø³Ù„Ø§Ù… Ø¯ÙˆØ³Øª Ø¹Ø²ÛŒØ²", "a": "Ø³Ù„Ø§Ù… Ø¹Ø²ÛŒØ² Ø¯Ù„! Ø®ÙˆØ´ Ø§ÙˆÙ…Ø¯ÛŒ"},
                    {"q": "Ø³Ù„Ø§Ù… Ø¬Ø§Ù†", "a": "Ø³Ù„Ø§Ù… Ø¬ÙˆÙ†Ù…! Ø­Ø§Ù„Øª Ú†Ø·ÙˆØ±Ù‡ØŸ"},
                ])
                
        all_data.extend(variations)
        
        print(f"ğŸ“Š Ù…Ø¬Ù…ÙˆØ¹ {len(all_data)} Ù…Ú©Ø§Ù„Ù…Ù‡ ØªÙˆÙ„ÛŒØ¯ Ø´Ø¯")
        return all_data
        
    def save_massive_dataset(self, data):
        """Ø°Ø®ÛŒØ±Ù‡ Ø¯ÛŒØªØ§Ø³Øª Ø¹Ø¸ÛŒÙ…"""
        print("ğŸ’¾ Ø°Ø®ÛŒØ±Ù‡ Ø¯ÛŒØªØ§Ø³Øª Ø¹Ø¸ÛŒÙ…...")
        
        # Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± ÙØ§ÛŒÙ„
        json_file = os.path.join(self.data_dir, "massive_dataset.json")
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        print(f"ğŸ“ Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± {json_file}")
        
        # Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± Ù…ØºØ² Fox
        profile = user_manager.get_current_user_profile()
        fox_learning = FoxLearningSystem(profile)
        
        saved_count = 0
        for item in data:
            try:
                fox_learning.teach_response(item["q"], item["a"])
                saved_count += 1
                
                if saved_count % 25 == 0:
                    print(f"âœ… {saved_count} Ù…Ú©Ø§Ù„Ù…Ù‡ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯...")
                    
            except Exception as e:
                continue
                
        print(f"ğŸ‰ {saved_count} Ù…Ú©Ø§Ù„Ù…Ù‡ Ø¯Ø± Ù…ØºØ² Fox Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯!")
        return saved_count
        
    def run_massive_download(self):
        """Ø§Ø¬Ø±Ø§ÛŒ Ø¯Ø§Ù†Ù„ÙˆØ¯ Ø¹Ø¸ÛŒÙ…"""
        print("ğŸš€ Ø´Ø±ÙˆØ¹ Ø¯Ø§Ù†Ù„ÙˆØ¯ Ø¯ÛŒØªØ§Ø³Øª Ø¹Ø¸ÛŒÙ…...")
        
        # ØªÙˆÙ„ÛŒØ¯ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§
        data = self.generate_massive_dataset()
        
        # Ø°Ø®ÛŒØ±Ù‡
        saved_count = self.save_massive_dataset(data)
        
        print("âœ… Ø¯Ø§Ù†Ù„ÙˆØ¯ Ø¹Ø¸ÛŒÙ… Ú©Ø§Ù…Ù„ Ø´Ø¯!")
        print(f"ğŸ“Š Ø¢Ù…Ø§Ø± Ù†Ù‡Ø§ÛŒÛŒ:")
        print(f"   - ØªÙˆÙ„ÛŒØ¯ Ø´Ø¯Ù‡: {len(data)} Ù…Ú©Ø§Ù„Ù…Ù‡")
        print(f"   - Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯Ù‡: {saved_count} Ù…Ú©Ø§Ù„Ù…Ù‡")
        
        return {
            "generated": len(data),
            "saved": saved_count
        }

# Global instance  
massive_downloader = MassiveDatasetDownloader()
