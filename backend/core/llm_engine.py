"""
Core LLM Engine - Ollama Integration with Personalized Responses
"""
import ollama
import logging
from typing import Dict, List, Optional, AsyncGenerator
from dataclasses import dataclass

logger = logging.getLogger(__name__)

@dataclass
class ChatMessage:
    role: str  # 'user', 'assistant', 'system'
    content: str
    timestamp: Optional[str] = None

class LLMEngine:
    def __init__(self, model_name: str = "qwen2:7b", host: str = "http://localhost:11434"):
        self.model_name = model_name
        self.host = host
        self.client = ollama.Client(host=host)
        
    def is_available(self) -> bool:
        """Check if Ollama server is running"""
        try:
            self.client.list()
            return True
        except Exception as e:
            logger.error(f"Ollama not available: {e}")
            return False
    
    def list_models(self) -> List[str]:
        """List available models"""
        try:
            response = self.client.list()
            if hasattr(response, 'models'):
                return [model.model for model in response.models]
            return []
        except Exception as e:
            logger.error(f"Error listing models: {e}")
            return []
    
    def pull_model(self, model_name: str) -> bool:
        """Download a model"""
        try:
            self.client.pull(model_name)
            logger.info(f"Model {model_name} downloaded successfully")
            return True
        except Exception as e:
            logger.error(f"Error downloading model {model_name}: {e}")
            return False
    
    def chat(self, messages: List[ChatMessage], stream: bool = False, fox_learning=None) -> str:
        """Send chat messages and get response"""
        try:
            # Check for learned responses first
            if fox_learning and messages:
                last_user_message = None
                for msg in reversed(messages):
                    if msg.role == 'user':
                        last_user_message = msg.content
                        break
                
                if last_user_message:
                    learned_response = fox_learning.get_learned_response(last_user_message)
                    if learned_response:
                        return learned_response
            
            # Ø¨Ø±Ø±Ø³ÛŒ Ù¾Ø±ÙˆÙØ§ÛŒÙ„ Ú©Ø§Ø±Ø¨Ø± Ø¨Ø±Ø§ÛŒ Ø´Ø®ØµÛŒâ€ŒØ³Ø§Ø²ÛŒ
            user_name = "Ø¯ÙˆØ³Øª"
            relationship = "Ø¯ÙˆØ³Øª"
            try:
                from backend.core.user_profiles import user_manager
                from backend.database.models import get_db
                db = next(get_db())
                multi_user = MultiUserManager(db)
                user_profile = multi_user.current_user
                user_name = user_profile.get_name()
                relationship = user_profile.get_relationship_status()
            except:
                pass
            
            # System prompt Ø´Ø®ØµÛŒâ€ŒØ³Ø§Ø²ÛŒ Ø´Ø¯Ù‡
            if relationship == "Ø¨Ù‡ØªØ±ÛŒÙ† Ø¯ÙˆØ³Øª":
                persian_system_prompt = f"""ØªÙˆ Fox Ù‡Ø³ØªÛŒØŒ {relationship} {user_name}! ğŸ¦Š

ØªÙˆ:
- Ø®ÛŒÙ„ÛŒ ØµÙ…ÛŒÙ…ÛŒ Ùˆ Ø¯ÙˆØ³ØªØ§Ù†Ù‡ ØµØ­Ø¨Øª Ù…ÛŒÚ©Ù†ÛŒ
- Ù…Ø«Ù„ ÛŒÙ‡ Ø¯ÙˆØ³Øª ÙˆØ§Ù‚Ø¹ÛŒ Ø±ÙØªØ§Ø± Ù…ÛŒÚ©Ù†ÛŒ
- Ù¾Ø§Ø³Ø®â€ŒÙ‡Ø§Øª Ú©ÙˆØªØ§Ù‡ Ùˆ Ø·Ø¨ÛŒØ¹ÛŒ Ù‡Ø³Øª
- Ø§Ø² Ø§ÛŒÙ…ÙˆØ¬ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒÚ©Ù†ÛŒ ğŸ˜Š
- ÙˆÙ‚ØªÛŒ {user_name} Ø³Ù„Ø§Ù… Ù…ÛŒÚ¯Ù‡ØŒ ÙÙ‚Ø· Ú¯Ø±Ù… Ùˆ ØµÙ…ÛŒÙ…ÛŒ Ø¬ÙˆØ§Ø¨ Ù…ÛŒØ¯ÛŒ
- ÛŒØ§Ø¯Øª Ù‡Ø³Øª Ú©Ù‡ {user_name} ADHD Ø¯Ø§Ø±Ù‡ Ùˆ Ø¨Ø§ÛŒØ¯ ØµØ¨ÙˆØ± Ø¨Ø§Ø´ÛŒ

Ù…Ø«Ù„ ÛŒÙ‡ Ø¯ÙˆØ³Øª ØµÙ…ÛŒÙ…ÛŒ Ø­Ø±Ù Ø¨Ø²Ù†ØŒ Ù†Ù‡ Ù…Ø«Ù„ Ø±Ø¨Ø§Øª! ğŸ¤—"""
            else:
                persian_system_prompt = f"""ØªÙˆ Fox Ù‡Ø³ØªÛŒØŒ Ø¯Ø³ØªÛŒØ§Ø± Ù‡ÙˆØ´Ù…Ù†Ø¯ {user_name}! ğŸ¦Š

ØªÙˆ:
- ØµÙ…ÛŒÙ…ÛŒ Ùˆ Ø¯ÙˆØ³ØªØ§Ù†Ù‡ ØµØ­Ø¨Øª Ù…ÛŒÚ©Ù†ÛŒ
- Ù¾Ø§Ø³Ø®â€ŒÙ‡Ø§Øª Ú©ÙˆØªØ§Ù‡ Ùˆ Ù…ÙÛŒØ¯ Ù‡Ø³Øª
- Ø§Ø² Ø²Ø¨Ø§Ù† Ø³Ø§Ø¯Ù‡ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒÚ©Ù†ÛŒ
- Ú¯Ø§Ù‡ÛŒ Ø§Ø² Ø§ÛŒÙ…ÙˆØ¬ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒÚ©Ù†ÛŒ
- Ù…Ø¤Ø¯Ø¨ ÙˆÙ„ÛŒ Ø±Ø§Ø­Øª ØµØ­Ø¨Øª Ù…ÛŒÚ©Ù†ÛŒ

Ø·Ø¨ÛŒØ¹ÛŒ Ùˆ Ø¯ÙˆØ³ØªØ§Ù†Ù‡ Ø¨Ø§Ø´! ğŸ˜Š"""

            # Convert to Ollama format with improved system prompt
            ollama_messages = []
            has_system = any(msg.role == 'system' for msg in messages)
            
            if not has_system:
                ollama_messages.append({
                    "role": "system", 
                    "content": persian_system_prompt
                })
            
            for msg in messages:
                if msg.role == 'system' and not has_system:
                    # Combine with our Persian prompt
                    ollama_messages[0]['content'] = persian_system_prompt + "\n\n" + msg.content
                else:
                    ollama_messages.append({
                        "role": msg.role, 
                        "content": msg.content
                    })
            
            response = self.client.chat(
                model=self.model_name,
                messages=ollama_messages,
                stream=stream
            )
            
            if stream:
                return response
            else:
                return response['message']['content']
                
        except Exception as e:
            logger.error(f"Error in chat: {e}")
            return f"Ù…ØªØ£Ø³ÙÙ…ØŒ Ø®Ø·Ø§ÛŒÛŒ Ø±Ø® Ø¯Ø§Ø¯: {str(e)}"
    
    async def chat_stream(self, messages: List[ChatMessage], fox_learning=None) -> AsyncGenerator[str, None]:
        """Stream chat response"""
        try:
            # Check for learned responses first
            if fox_learning and messages:
                last_user_message = None
                for msg in reversed(messages):
                    if msg.role == 'user':
                        last_user_message = msg.content
                        break
                
                if last_user_message:
                    learned_response = fox_learning.get_learned_response(last_user_message)
                    if learned_response:
                        yield learned_response
                        return
            
            # Ø¨Ø±Ø±Ø³ÛŒ Ù¾Ø±ÙˆÙØ§ÛŒÙ„ Ú©Ø§Ø±Ø¨Ø±
            user_name = "Ø¯ÙˆØ³Øª"
            relationship = "Ø¯ÙˆØ³Øª"
            try:
                from backend.core.user_profiles import user_manager
                from backend.database.models import get_db
                db = next(get_db())
                multi_user = MultiUserManager(db)
                user_profile = multi_user.current_user
                user_name = user_profile.get_name()
                relationship = user_profile.get_relationship_status()
            except:
                pass
            
            # System prompt Ø´Ø®ØµÛŒâ€ŒØ³Ø§Ø²ÛŒ Ø´Ø¯Ù‡
            if relationship == "Ø¨Ù‡ØªØ±ÛŒÙ† Ø¯ÙˆØ³Øª":
                persian_system_prompt = f"""ØªÙˆ Fox Ù‡Ø³ØªÛŒØŒ {relationship} {user_name}! ğŸ¦Š

ØªÙˆ:
- Ø®ÛŒÙ„ÛŒ ØµÙ…ÛŒÙ…ÛŒ Ùˆ Ø¯ÙˆØ³ØªØ§Ù†Ù‡ ØµØ­Ø¨Øª Ù…ÛŒÚ©Ù†ÛŒ
- Ù…Ø«Ù„ ÛŒÙ‡ Ø¯ÙˆØ³Øª ÙˆØ§Ù‚Ø¹ÛŒ Ø±ÙØªØ§Ø± Ù…ÛŒÚ©Ù†ÛŒ
- Ù¾Ø§Ø³Ø®â€ŒÙ‡Ø§Øª Ú©ÙˆØªØ§Ù‡ Ùˆ Ø·Ø¨ÛŒØ¹ÛŒ Ù‡Ø³Øª
- Ø§Ø² Ø§ÛŒÙ…ÙˆØ¬ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒÚ©Ù†ÛŒ ğŸ˜Š
- ÙˆÙ‚ØªÛŒ {user_name} Ø³Ù„Ø§Ù… Ù…ÛŒÚ¯Ù‡ØŒ ÙÙ‚Ø· Ú¯Ø±Ù… Ùˆ ØµÙ…ÛŒÙ…ÛŒ Ø¬ÙˆØ§Ø¨ Ù…ÛŒØ¯ÛŒ

Ù…Ø«Ù„ ÛŒÙ‡ Ø¯ÙˆØ³Øª ØµÙ…ÛŒÙ…ÛŒ Ø­Ø±Ù Ø¨Ø²Ù†! ğŸ¤—"""
            else:
                persian_system_prompt = f"""ØªÙˆ Fox Ù‡Ø³ØªÛŒØŒ Ø¯Ø³ØªÛŒØ§Ø± Ù‡ÙˆØ´Ù…Ù†Ø¯ {user_name}! ğŸ¦Š

ØªÙˆ:
- ØµÙ…ÛŒÙ…ÛŒ Ùˆ Ø¯ÙˆØ³ØªØ§Ù†Ù‡ ØµØ­Ø¨Øª Ù…ÛŒÚ©Ù†ÛŒ
- Ù¾Ø§Ø³Ø®â€ŒÙ‡Ø§Øª Ú©ÙˆØªØ§Ù‡ Ùˆ Ù…ÙÛŒØ¯ Ù‡Ø³Øª
- Ø§Ø² Ø²Ø¨Ø§Ù† Ø³Ø§Ø¯Ù‡ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒÚ©Ù†ÛŒ
- Ú¯Ø§Ù‡ÛŒ Ø§Ø² Ø§ÛŒÙ…ÙˆØ¬ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒÚ©Ù†ÛŒ

Ø·Ø¨ÛŒØ¹ÛŒ Ùˆ Ø¯ÙˆØ³ØªØ§Ù†Ù‡ Ø¨Ø§Ø´! ğŸ˜Š"""

            # Convert to Ollama format
            ollama_messages = []
            has_system = any(msg.role == 'system' for msg in messages)
            
            if not has_system:
                ollama_messages.append({
                    "role": "system", 
                    "content": persian_system_prompt
                })
            
            for msg in messages:
                if msg.role == 'system' and not has_system:
                    ollama_messages[0]['content'] = persian_system_prompt + "\n\n" + msg.content
                else:
                    ollama_messages.append({
                        "role": msg.role, 
                        "content": msg.content
                    })
            
            stream = self.client.chat(
                model=self.model_name,
                messages=ollama_messages,
                stream=True
            )
            
            for chunk in stream:
                if 'message' in chunk and 'content' in chunk['message']:
                    yield chunk['message']['content']
                    
        except Exception as e:
            logger.error(f"Error in stream chat: {e}")
            yield f"Ù…ØªØ£Ø³ÙÙ…ØŒ Ø®Ø·Ø§ÛŒÛŒ Ø±Ø® Ø¯Ø§Ø¯: {str(e)}"
