"""
Multi-Model Learning System - ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø§Ø² Ú†Ù†Ø¯ Ù…Ø¯Ù„
"""
from backend.core.api_manager import api_manager
from backend.core.llm_engine import LLMEngine, ChatMessage
import json
import asyncio

class MultiModelLearning:
    def __init__(self, fox_learning):
        self.fox_learning = fox_learning
        self.llm = LLMEngine()
    
    async def get_multi_model_response(self, question: str, models: list = None):
        """Ø¯Ø±ÛŒØ§ÙØª Ù¾Ø§Ø³Ø® Ø§Ø² Ú†Ù†Ø¯ Ù…Ø¯Ù„ Ùˆ ØªØ±Ú©ÛŒØ¨ Ø¢Ù†Ù‡Ø§"""
        if not models:
            models = ['ollama', 'groq', 'huggingface']  # Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ù¾ÛŒØ´â€ŒÙØ±Ø¶
        
        responses = {}
        
        # Ø¯Ø±ÛŒØ§ÙØª Ù¾Ø§Ø³Ø® Ø§Ø² Ù…Ø¯Ù„ Ù…Ø­Ù„ÛŒ (Ollama)
        try:
            messages = [ChatMessage("user", question)]
            ollama_response = self.llm.chat(messages)
            responses['ollama'] = ollama_response
        except:
            responses['ollama'] = "Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø±ÛŒØ§ÙØª Ù¾Ø§Ø³Ø®"
        
        # Ø¯Ø±ÛŒØ§ÙØª Ù¾Ø§Ø³Ø® Ø§Ø² API Ù‡Ø§ÛŒ Ø®Ø§Ø±Ø¬ÛŒ
        for model in models:
            if model != 'ollama' and model in api_manager.apis:
                try:
                    api_response = api_manager.chat_with_api(model, [{"role": "user", "content": question}])
                    responses[model] = api_response
                except:
                    responses[model] = "Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø±ÛŒØ§ÙØª Ù¾Ø§Ø³Ø®"
        
        return responses
    
    def analyze_responses(self, responses: dict):
        """ØªØ­Ù„ÛŒÙ„ Ùˆ Ù…Ù‚Ø§ÛŒØ³Ù‡ Ù¾Ø§Ø³Ø®â€ŒÙ‡Ø§"""
        analysis = {
            "best_response": "",
            "common_points": [],
            "unique_insights": {},
            "quality_scores": {}
        }
        
        # Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Ù†Ú©Ø§Øª Ù…Ø´ØªØ±Ú©
        all_responses = list(responses.values())
        
        # Ø§Ù†ØªØ®Ø§Ø¨ Ø¨Ù‡ØªØ±ÛŒÙ† Ù¾Ø§Ø³Ø® (Ø³Ø§Ø¯Ù‡â€ŒØªØ±ÛŒÙ†: Ø·ÙˆÙ„Ø§Ù†ÛŒâ€ŒØªØ±ÛŒÙ† Ù¾Ø§Ø³Ø® Ù…Ø¹Ù‚ÙˆÙ„)
        best_key = max(responses.keys(), key=lambda k: len(responses[k]) if len(responses[k]) < 1000 else 0)
        analysis["best_response"] = responses[best_key]
        
        # Ø°Ø®ÛŒØ±Ù‡ Ø¨Ø±Ø§ÛŒ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ
        analysis["responses"] = responses
        
        return analysis
    
    def learn_from_multi_model(self, question: str, analysis: dict):
        """ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø§Ø² Ù¾Ø§Ø³Ø®â€ŒÙ‡Ø§ÛŒ Ú†Ù†Ø¯Ú¯Ø§Ù†Ù‡"""
        # Ø°Ø®ÛŒØ±Ù‡ Ø¨Ù‡ØªØ±ÛŒÙ† Ù¾Ø§Ø³Ø®
        self.fox_learning.add_learned_response(question, analysis["best_response"])
        
        # Ø°Ø®ÛŒØ±Ù‡ ØªØ­Ù„ÛŒÙ„ Ø¨Ø±Ø§ÛŒ Ø¨Ù‡Ø¨ÙˆØ¯ Ø¢ÛŒÙ†Ø¯Ù‡
        learning_data = {
            "question": question,
            "multi_responses": analysis["responses"],
            "selected_best": analysis["best_response"],
            "timestamp": str(datetime.now())
        }
        
        # Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± ÙØ§ÛŒÙ„ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ
        self.save_learning_data(learning_data)
    
    def save_learning_data(self, data):
        """Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ"""
        import os
        learning_file = "data/multi_model_learning.json"
        
        if os.path.exists(learning_file):
            with open(learning_file, 'r', encoding='utf-8') as f:
                existing_data = json.load(f)
        else:
            existing_data = []
        
        existing_data.append(data)
        
        # Ù†Ú¯Ù‡ Ø¯Ø§Ø´ØªÙ† ÙÙ‚Ø· 100 Ù…ÙˆØ±Ø¯ Ø¢Ø®Ø±
        if len(existing_data) > 100:
            existing_data = existing_data[-100:]
        
        os.makedirs(os.path.dirname(learning_file), exist_ok=True)
        with open(learning_file, 'w', encoding='utf-8') as f:
            json.dump(existing_data, f, indent=2, ensure_ascii=False)
    
    def get_smart_response(self, question: str):
        """Ù¾Ø§Ø³Ø® Ù‡ÙˆØ´Ù…Ù†Ø¯ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ú†Ù†Ø¯ Ù…Ø¯Ù„"""
        # Ø§Ø¨ØªØ¯Ø§ Ú†Ú© Ú©Ù† Ø¢ÛŒØ§ Ù‚Ø¨Ù„Ø§Ù‹ ÛŒØ§Ø¯ Ú¯Ø±ÙØªÙ‡
        learned = self.fox_learning.get_learned_response(question)
        if learned:
            return f"ğŸ§  (Ø§Ø² ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ù‚Ø¨Ù„ÛŒ): {learned}"
        
        # Ø§Ú¯Ø± Ù†Ù‡ØŒ Ø§Ø² Ú†Ù†Ø¯ Ù…Ø¯Ù„ Ø¨Ù¾Ø±Ø³
        responses = asyncio.run(self.get_multi_model_response(question))
        analysis = self.analyze_responses(responses)
        
        # ÛŒØ§Ø¯ Ø¨Ú¯ÛŒØ± Ø¨Ø±Ø§ÛŒ Ø¯ÙØ¹Ù‡ Ø¨Ø¹Ø¯
        self.learn_from_multi_model(question, analysis)
        
        return f"ğŸ¤– (ØªØ±Ú©ÛŒØ¨ Ú†Ù†Ø¯ Ù…Ø¯Ù„): {analysis['best_response']}"

# Global instance
multi_model_learning = None
